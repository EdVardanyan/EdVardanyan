import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score

# 1.Data prep.
# dataset loading
url = "https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv"
df = pd.read_csv(url)

# with the help of this functions combination, we can find out which columns contain NaN values
df.isna().any()

# data cleaning

# drop columns which according to me are redundant for future predictions (can be different)
# thanks to inplace=True, we do mofications inplace, instead of returning copy of modified df
df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'], inplace=True)

# there are some places where age is Nan, so we change its value with 
df['Age'].fillna(df['Age'].median(), inplace=True)

# subscript operator is used because mode returns an array of modes
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

# encoding categorical variables

label_encoder = LabelEncoder()
# Sex is transformed into 0s and 1s
# df['Sex'] = label_encoder.fit_transform(df['Sex'])

# another way for encoding is this
df['Sex'] = df['Sex'].map({"male": 0, "female": 1})

df['Embarked'] = label_encoder.fit_transform(df['Embarked'])

# Feature Engineering
df['Family_Size'] = df['SibSp'] + df['Parch']
df.drop(columns=['SibSp', 'Parch'], inplace=True)

# 2.EDA

df.describe()
# in the result of describe, count indicates the number of non-null values in the column
# percentages show x% of data points that fell bellow some value Y


# gives helpful insights for undestanding the distribution of values in a column, identifying 
# the most common categories
df.value_counts()

# the following plots will both show distribution and will visualize

sns.countplot(x='Survived', data=df)
plt.show()

# this will show count of people who survived and did not according to their gender
# which will show that a few men survived, and few women did not survive
sns.countplot(x='Sex', hue='Survived', data=df)
plt.show()

# this will which class of people mostly survived
sns.countplot(x='Pclass', hue='Survived', data=df)
plt.show()

# Correlations

# shows  the strength and direction of a relationship between two variables
corr_matrix = df.corr()
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm")
plt.show()

# 3.ML Models training
# here we drop survived and return the copy without Survived into X, for predicting the actual
# number of Survived people according to independent variables
X = df.drop(columns=['Survived'])
y = df['Survived']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

models = {
    "Log-Reg": LogisticRegression(),
    "Random-Forest": RandomForestClassifier(),
    "Gradient-Boosting": GradientBoostingClassifier() 
}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    f1_res = f1_score(y_test, y_pred)
    print(f1_res)

# after getting f1_scores we find out that RandomForest is the most suitable supervised algorithm
# for making predictions in this case

# 4.Models evaluation and tuning
rf_param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 20, None]
}

# hyperparameter tunning is for finding the best hyperparameters with which our model will demonstrate its whole potential
# ու ստեղ մի 3 տեսակով պետք ա անել, բայց նաև հաշվի առեք, որ կախված մոդելից հիպերպարամետրերը տարբեր են

rf_grid_search = GridSearchCV(RandomForestClassifier(), rf_param_grid, cv=5, n_jobs=4)
rf_grid_search.fit(X_train, y_train)
print("Best params: ", rf_grid_search.best_params_)

best_rf_model = rf_grid_search.best_estimator_
print(best_rf_model)

y_pred_rf = best_rf_model.predict(X_test)
f1_rf = f1_score(y_test, y_pred_rf)

print("f1 rf:", f1_rf)
print("Classification Report:")
print(classification_report(y_test, y_pred_rf))
sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True)
plt.show()

lr_param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}
lr_grid_search = GridSearchCV(LogisticRegression(max_iter=100), lr_param_grid, cv=5, n_jobs=4)
lr_grid_search.fit(X_train, y_train)
print("best params for LR ", lr_grid_search.best_params_)

gb_param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.05, 0.1, 0.5, 1],
    'max_depth': [3, 4, 5]
}

gb_grid_search = GridSearchCV(GradientBoostingClassifier(), gb_param_grid, cv=5)
gb_grid_search.fit(X_train, y_train)
print("parameters ", gb_grid_search.best_params_)

# և այլն, էլ նույն բանը չգրեմ